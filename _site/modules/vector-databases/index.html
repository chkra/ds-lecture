<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Vektor Datenbanken -</title>
<meta name="description" content="Neue Impulse für die KI-Lehre">


  <meta name="author" content="Christina Kratsch">
  


<meta property="og:type" content="website">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="">
<meta property="og:title" content="Vektor Datenbanken">
<meta property="og:url" content="http://localhost:4000/modules/vector-databases/">


  <meta property="og:description" content="Neue Impulse für die KI-Lehre">











  

  


<link rel="canonical" href="http://localhost:4000/modules/vector-databases/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "",
      "url": "http://localhost:4000/"
    
  }
</script>







<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title=" Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single wide">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
          <a class="site-logo" href="/"><img src="/assets/images/logo.png" alt=""></a>
        
        <a class="site-title" href="/">
          
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="https://mmistakes.github.io/minimal-mistakes/docs/quick-start-guide/">Quick-Start Guide</a>
            </li><li class="masthead__menu-item">
              <a href="https://mmistakes.github.io/minimal-mistakes/about/">About</a>
            </li><li class="masthead__menu-item">
              <a href="/year-archive/">Sample Posts</a>
            </li><li class="masthead__menu-item">
              <a href="/collection-archive/">Sample Collections</a>
            </li><li class="masthead__menu-item">
              <a href="/sitemap/">Sitemap</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main">
  
  <div class="sidebar sticky">
  



<div style="color: #76B900;margin-bottom:-10px;font-weight: bold;"><h3>Data Science</h3></div>

<div style="color: #858585;font-size:small;">Grundlegende Methoden für die Exploration und das Management von Daten.</div>




<a href="https://creativecommons.org/licenses/by/4.0/">
  <img style="height: 15px; width: 80px;" src="https://licensebuttons.net/l/by/4.0/80x15.png">
</a>


<div style="color: #858585;font-size:small;margin-top: -15px;">2024 | HTW Berlin</div>




<div itemscope itemtype="https://schema.org/Person" class="h-card">

  
    <div class="author__avatar">
      <a href="http://localhost:4000/">
        <img src="/assets/images/authors/ck.png" alt="Christina Kratsch" itemprop="image" class="u-photo">
      </a>
    </div>
  

  <div class="author__content">
    <div class="author__name p-name" itemprop="name">
      <a class="u-url" rel="me" href="http://localhost:4000/" itemprop="url">Christina Kratsch</a>
    </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      

      
        
          
            <li><a href="mailto:kratsch@htw-berlin.de" rel="nofollow noopener noreferrer me"><i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i><span class="label">Email</span></a></li>
          
        
          
            <li><a href="https://www.htw-berlin.de/hochschule/personen/person/?eid=13856" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fas fa-fw fa-link" aria-hidden="true"></i><span class="label">Website</span></a></li>
          
        
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer me">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>



  <article class="page h-entry" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Vektor Datenbanken">
    
    
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title p-name" itemprop="headline">
            <a href="http://localhost:4000/modules/vector-databases/" class="u-url" itemprop="url">Vektor Datenbanken
</a>
          </h1>
          


        </header>
      

      <section class="page__content e-content" itemprop="text">
        
        <p>Vektordatenbanken spielen schon seit langer Zeit in der Geoinformationstechnologie eine große Rolle. Im Gegensatz zu herkömmlichen relationalen Datenbanken setzen Vektordatenbanken auf die Speicherung in Form von Vektoren, die Punkte, Linien oder Polygone repräsentieren. Dies ermöglicht eine präzise Modellierung und Analyse räumlicher Informationen. Durch die Kombination von KI-Algorithmen mit (Echtzeit-)Geoinformationen sind in der Vergangenheit innovative standortbasierten Dienste, Navigationssysteme und städtische Planungslösungen entstanden.</p>

<p>Besondere Aufmerksamkeit haben Vektordatenbanken auch mit dem Aufkommen von ChatGPT &amp; Co erlangt. Large Language Models haben zahlreiche neue und aufregende Anwendungen ermöglicht.Ene bekannte Einschränkung von ChatGPT ist, dass ein trainiertes Sprachmodell keine Kenntnisse über aktuelle Ereignisse hat und keine Informationen aus Dokumenten besitzt, auf denen es nicht trainiert wurde. Um dieses Problem zu bewältigen, kommt die Methode der <strong>Retrieval Augmented Generation</strong>  (RAG) zum Einsatz, für die Vektordatenbanken unerlässlich sind.</p>

<h2 id="vector-embeddings-mit-neuronalen-netzen">Vector Embeddings mit neuronalen Netzen</h2>

<p>Um alltägliche Daten als Vektoren abzubilden, werden verschiedene Methoden genutzt. Im folgenden Beispiel nutzen wir einen Autoencoder, um Bilddaten erst auf eine niedrig-dimensionalere Repräsentation abzubilden (Encoder) und dann (fehlerbehaftet) die Originaldaten wieder herzustellen. Nachdem das neuronale Netz wiederholt trainiert wurde, so dass der Rekonstruktionsfehler minimiert wurde, kann der innere <em>Latent Space</em> als niedrigdimensionaleres Embedding der Originaldaten verstanden werden. Der Encoder ist also unser <em>Vecotrizer</em>, der Latent Space unser Ziel-Vektorraum, und der Decoder die Rückübersetzung in den Bildraum.</p>

<p><img src="/modules/vector-databases/images/Autoencoder.png" alt="" />
<em>Autoencoder (schematischer Aufbau)</em></p>

<p><a href="https://databasecamp.de/ki/autoencoder">Dieser Code</a> zeigt exemplarisch, wie man einen Autoencoder für MNIST-Daten trainieren kann. Es fällt insbesondere auf, dass ähnliche Bilder (z.B. Bilder der handschriftlichen Zahl “0”) auf ähnliche (nahe) Vektoren im Latent Space abgebildet werden. Dieses Prinzip lässt sich insbesondere mit Bilddaten, aber auch mit allen numerischen Eingabedaten fester Größe verwenden. Text-Eingaben, wie sie bei LLMs verwendet werden, zeichnen sich insbesondere durch ihre dynamsische Länge aus, deshalb wird das Prinzip des Autoencoders abgewandelt. Ein wichtiger Ansatz dafür ist word2vec, den wir an anderer Stelle erklären.</p>

<p>@Todo
<!-- 
How TO Word2Vec
https://www.tensorflow.org/text/tutorials/word2vec
--></p>

<h2 id="semantische-suche-k-nearest-neighbor-im-latent-space">Semantische Suche: k-Nearest Neighbor im Latent Space</h2>

<p>Ein erster Anwendungsfall von vektorisierter Datenrepräsenation liegt nahe. Stellen Sie sich eine Wissensbasis vor (z.B. Website, Sharepoint, Dokumentensammlung), die mit Hilfe eines Autoencoders als Menge von Vektoren abgelegt wurde. Wir können nun mit einer Anfrage (Query) in natürlicher Sprache in dieser “Datenbank” suchen, indem wir unseren Query als Vektor darstellen und die $k$ nächsten (ähnlichsten) Dokumentvektoren als Ergebnis der Suche ausgeben. Dazu berechnen wir die Distanzen des Vektors zu allen Einträgen (= Vektoren = Punkten) in der Datenbank und geben die <em>k-Nearest-Neighbors</em> aus.</p>

<p><img src="/modules/vector-databases/images/SemanticSearch.png" alt="" />
<em>Semantische Suche: Distanzen vom Query zu allen Vektoren in der Datenbank)</em></p>

<h2 id="approximate-knn-vektordatenbanken">Approximate $k$NN: Vektordatenbanken</h2>

<p>Leider sinkt die Performance einer brute force Suche stark sowohl mit der Dimension des Latent Spaces als auch der Größe der Datenbank.</p>

<p>@Todo: 
Übungsaufgabe: Was ist die Komplexitätsklasse einer brute force Semantischen Suche eines $k$-Nearest-Neighbor-Algorithmus mit Euklidscher Distanz in einem D-dimensionalen Vektorraum mit N bestehenden Vektoren?</p>

<!-- 
Die Komplexitätsklasse einer brute force Suche eines k-Nearest-Neighbor-Algorithmus mit Euklidscher Distanz in einem D-dimensionalen Vektorraum mit N bestehenden Vektoren ist O(ND)1. Das bedeutet, dass die Laufzeit des Algorithmus proportional zur Anzahl der Dimensionen D und zur Anzahl der Vektoren N ist. Um die Klassifizierung eines neuen Punkts zu bestimmen, muss der Algorithmus die euklidische Distanz zwischen diesem Punkt und allen anderen Punkten im Datensatz berechnen2. Dies erfordert D Multiplikationen und D-1 Additionen für jeden Punkt, also insgesamt ND Multiplikationen und N(D-1) Additionen. Die Komplexität hängt also nur von N und D ab, nicht von k3.
-->

<p>Zum Glück existiert eine Reihe von Approximationsalgorithmen, die das Ergebnis deutlich beschleunigen, zum Beispiel</p>
<ul>
  <li>Navigable Small World Search</li>
  <li>Hierarchical Navigable Small World (HNSW) Search
welche eine Laufzeit von nur $O(\log N)$ aufweisen.</li>
</ul>

<p>Diese Approximationsalgorithmen legen zusätzliche Metadaten im Vektorraum an, die die Suche beschleunigen. Diese Vorgehensweise macht den Kern von <em>Vektordatenbanken</em> aus, welche Sammlungen von Vektoren verwalten und schnell durchsuchbar machen.</p>

<p>Das beigefügte Jupyter Notebook (abgewandelt vom Original-Kurs, siehe Quellenangaben) zeigt, wie man eine Vektordatenbank mit Daten füllt und einen Query gegen die Datenbank laufen lässt.</p>

<p>@todo
<!-- 
https://learn.deeplearning.ai/vector-databases-embeddings-applications/lesson/4/approximate-nearest-neighbours
--></p>

<h2 id="retrieval-augmented-generation">Retrieval Augmented Generation</h2>

<p>Retrieval Augmented Generation (RAG) beschreibt den Ansatz, zusätzliches Wissen in einer Vektor-Datenbank als externe Knowledge Base für einen Query an ein Large Language Model zu verwenden. Die Vorteile sind vielfältig:</p>
<ul>
  <li>das LLM erhält zusätzliche (aktuelle) Fakten für die Formulierung seiner Antworten</li>
  <li>Prompts an das LLM können mit zusätzlichen Informationen ausgestattet werden</li>
  <li>spart Re-Training oder Finetuning des LLMs</li>
  <li>reduziert Halluzinationen</li>
  <li>ermöglicht dem LLM das Nennen von Quellen</li>
</ul>

<p>Die folgende Grafik illustriert den Ablauf einer LLM-Anfrage mit RAG:</p>

<ol>
  <li>Ein Nutzer formuliert eine Anfrage.</li>
  <li>Das System sucht in einer Knowledge Base (Vektordatenbank, aber auch externe Quellen) nach relevanten Dokumenten zur Anfrage.</li>
  <li>Diese Dokumente werden (vektorisiert) der Anfrage an das LLM beigefügt.</li>
  <li>Das LLM hat mehr Kontext für seine Anfrage und kann so eine qualitativ hochwertigere Antwort geben.</li>
</ol>

<p><img src="/modules/vector-databases/images/RAG.png" alt="" />
<em>RAG: schematischer Ablauf</em></p>

<p><a href="https://learn.deeplearning.ai/vector-databases-embeddings-applications/lesson/7/application---multilingual-search">Dieser Code</a> zeigt zwei naheliegende Anwendungen des RAG-Prinzips:</p>
<ol>
  <li>Iteratives single prompting: Beauftrage wiederholt das LLM für jedes einzelne Dokument im Kontext: <em>Hallo ChatGPT, schreibe mir einen facebook post über {title} mit Informationen über {content}</em>.</li>
  <li>Gruppen-Prompt: <em>Hallo ChatGPT, erstelle mir eine Zusammenfassung</em></li>
</ol>

<h2 id="weiterführende-infos-und-selbststudium">Weiterführende Infos und Selbststudium</h2>

<h2 id="-quellenangaben">⚑ Quellenangaben</h2>

<p class="notice--info">Dieses Modul wurde auf Basis des Kurses <em>Vector Databases: from Embeddings to Applications</em> von Andrew Ng (DeepLearning.AI) und Sebastian Witalec (Weaviate) erstellt. <a href="https://www.deeplearning.ai/short-courses/vector-databases-embeddings-applications">Zum Kurs</a></p>


        
      </section>

      <footer class="page__meta">
        
        


        

      </footer>

      

      
    </div>

    
  </article>

  
  
</div>

    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
      
        
      
        
      
        
      
        
      
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2024 . Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>










  </body>
</html>
